FIP Question,DCS Field,maDMP Value,Allowed Values,Compliant
"What globally unique, persistent, resolvable identifiers do you use for metadata records?",dataset.dataset_id.identifier,"[""f55bf8d2-be76-4ef0-adb6-d14d6a17358d"", ""686ab52b-7688-4deb-8859-e2b0c792ff72""]",DataCite DOI Resolution Service,"[No, No]"
"What globally unique, persistent, resolvable identifiers do you use for datasets?",dataset.dataset_id.identifier,"[""f55bf8d2-be76-4ef0-adb6-d14d6a17358d"", ""686ab52b-7688-4deb-8859-e2b0c792ff72""]",DataCite DOI Resolution Service,"[No, No]"
Which metadata schemas do you use for findability?,dataset.metadata.metadata_standard_id.identifier,null,Schema.org,No
What is the technology that links the persistent identifiers of your data to the metadata description?,dataset.distribution.host.pid_system,null,DataCite,No
In which search engines are your metadata records indexed?,dataset.distribution.access_url,null,Google Dataset Search,No
In which search engines are your datasets indexed?,dataset.distribution.access_url,null,,No choice made by community
Which standardized communication protocol do you use for metadata records?,dataset.distribution.host.url,null,HTTPS,No
Which standardized communication protocol do you use for datasets?,dataset.distribution.host.url,null,,No choice made by community
Which authentication & authorisation technique do you use for metadata records?,dataset.distribution.data_access,null,Open Data,No
Which authentication & authorisation technique do you use for datasets?,dataset.distribution.data_access,null,,No choice made by community
Which metadata longevity plan do you use?,,null,,No choice made by community
Which knowledge representation languages (allowing machine interoperation) do you use for metadata records?,,null,JSON-LD,No
Which knowledge representation languages (allowing machine interoperation) do you use for datasets?,,null,CSV File Format ,No
Which structured vocabularies do you use to annotate your metadata records?,dataset.metadata.metadata_standard_id.identifier,null,Schema.org,No
Which structured vocabularies do you use to encode your datasets?,dataset.metadata.metadata_standard_id.identifier,null,OHDSI Observational Health Data Sciences and Informatics,No
"Which models, schema(s) do you use for your metadata records?",dataset.metadata.metadata_standard_id.type,null,Schema.org Dataset,No
"Which models, schema(s) do you use for your datasets?",dataset.metadata.metadata_standard_id.type,null,OHDSI Observational Health Data Sciences and Informatics,No
Which usage license do you use for your metadata records?,dataset.distribution.license.license_ref,null,CC BY 4.0,No
Which usage license do you use for your datasets?,dataset.distribution.license.license_ref,null,CC BY 4.0,No
Which metadata schemas do you use for describing the provenance of your metadata records?,dataset.data_quality_assurance,"[[""Set up of scientific and technical committee"", ""Use of tools for automatic checks"", ""Data conform to format specification"", ""Consistency verified with data models and standards"", ""BIGAN has a data quality assurance system for data processing to enable the secondary use of health data in Aragon (Spain).&#160; In addition, we implement our own data quality assessment and assurance procedures within the BY-COVID baseline use case, aiming to provide insight into the impact of data quality in interpreting the research outcomes and producing high-quality research. Those procedures include the assessment of the information requirements and the data model specification by a scientific and technical committee, the implementation of an automatic data quality check (i.e. exploratory data analysis - EDA) on the original dataset of each participant on-site, the implementation of data conformance and consistency checking following the common data model specification, and a final missing values assessment on core variables to inform decisions on imputation requirements.&#160;""], [""Set up of scientific and technical committee"", ""Use of tools for automatic checks"", ""Data conform to format specification"", ""Consistency verified with data models and standards"", ""We implement our own data quality assessment and assurance procedures within the BY-COVID baseline use case, aiming to provide insight into the impact of data quality in interpreting the research outcomes and producing high-quality research. Those procedures include the assessment of the information requirements and the data model specification by a scientific and technical committee, the implementation of an automatic data quality check (i.e. exploratory data analysis - EDA) on the original dataset (linked and transformed data to comply with the requirements captured in the common data model specification, conditional on data availability and access) of each participating node, the implementation of data conformance and consistency checking following the common data model specification, and a final missing values assessment on core variables to inform decisions on imputation requirements. The EDA, checking of consistency with the common data model specification and missing values assessment are performed within the secured processing environment (SPE) of each participating node.<br>""]]",Schema.org,"[No, No]"
Which metadata schemas do you use for describing the provenance of your datasets?,dataset.data_quality_assurance,"[[""Set up of scientific and technical committee"", ""Use of tools for automatic checks"", ""Data conform to format specification"", ""Consistency verified with data models and standards"", ""BIGAN has a data quality assurance system for data processing to enable the secondary use of health data in Aragon (Spain).&#160; In addition, we implement our own data quality assessment and assurance procedures within the BY-COVID baseline use case, aiming to provide insight into the impact of data quality in interpreting the research outcomes and producing high-quality research. Those procedures include the assessment of the information requirements and the data model specification by a scientific and technical committee, the implementation of an automatic data quality check (i.e. exploratory data analysis - EDA) on the original dataset of each participant on-site, the implementation of data conformance and consistency checking following the common data model specification, and a final missing values assessment on core variables to inform decisions on imputation requirements.&#160;""], [""Set up of scientific and technical committee"", ""Use of tools for automatic checks"", ""Data conform to format specification"", ""Consistency verified with data models and standards"", ""We implement our own data quality assessment and assurance procedures within the BY-COVID baseline use case, aiming to provide insight into the impact of data quality in interpreting the research outcomes and producing high-quality research. Those procedures include the assessment of the information requirements and the data model specification by a scientific and technical committee, the implementation of an automatic data quality check (i.e. exploratory data analysis - EDA) on the original dataset (linked and transformed data to comply with the requirements captured in the common data model specification, conditional on data availability and access) of each participating node, the implementation of data conformance and consistency checking following the common data model specification, and a final missing values assessment on core variables to inform decisions on imputation requirements. The EDA, checking of consistency with the common data model specification and missing values assessment are performed within the secured processing environment (SPE) of each participating node.<br>""]]",PROV-O,"[No, No]"
